---  
type: TutorialClass  
title: 0.3-Docker-Primer  
description: "It is imperative you become acquainted with Docker and its  
  concepts before we start the project. We will discuss the vocab, configuration  
  methods, and caveats that may trip you up. "  
share: true  
module: 0-Getting-Started  
created: Tuesday, May 16th 2023, 2:34:00 pm  
modified: Friday, July 14th 2023, 2:26:46 am  
tags:  
  - shapecreations  
  - docker  
  - container  
  - image  
status: complete  
---  
  
  
---  
  
# 0.3-Docker-Primer  
  
![](https://img.shields.io/badge/-Docker-2496ED?logo=docker&logoColor=white&style=plastic)  
  
---  
  
## 🎯 Lesson Objective  
  
> **By the end of this lesson you should be able to:**  
> - Describe what Docker is, its use cases, and how it relates to Shape Creations.  
> - Define the key concepts of Docker including: images, containers, layers, and the difference between build-time and run-time.  
> - Discuss what makes Docker different from installing/running services on your host computer or that of other containerization solutions.  
> - Install Docker Desktop on your computer and register for a free Docker Hub account.  
  
## ⛰ Overview  
  
[Docker](https://www.docker.com/) is a one-stop shop for any project. Be it complex with lots of moving parts or a single service utility, Docker has enough flexibility and scalability to meet your needs. In this section, we will explore the theory behind Docker containerization and gain a sense for its sometimes fickle behavior.  
  
We start with a  
  
> 🔍 See Also    
>Jumping into Docker without formal coursework is rough! I spent months of my life struggling to piece together tutorials and documentation for this project. I became a script-kitty and still couldn’t fit the puzzle together.  
>  
>Finally, I took a Udemy course called [Docker & Kubernetes: The Practical Guide, 2023 Edition](https://www.udemy.com/share/103Ia0/). I cannot recommend the course enough (and no I’m not paid to say that!). I’ll tell you right now, I won’t be able to accomplish the same level of teaching proficiency when writing this primer. If you can, find a tutorial like this one. It literally (rather figuratively) takes the witchcraft out of Docker.  
>  
>Much of this module section will be gleaned from my notes during this Udemy course. As such I have provided the course as a reference for this entire document (see the [Resources](0.3-Docker-Primer.md#resources) section).[^1]  
  
## 📓 Learning the Lingo  
  
Before I explain why you should be giddy about Docker, knowing a little of the lingo will help demystify the tool as a concept. There are many *Dockerisms* that need explaining but we will start with what a project built with Docker looks like.  
  
A Docker project is a standalone instance of code that can be run on any machine regardless if the computer it’s running on has the needed programs or tools installed. Another computer need only Docker installed to run that code. This means, you can build an application on a Mac and pass it to a colleague running Windows or Linux (and vise versa) and it will run, without any additional configuration or translation, just the way it ran on original computer.  
  
This is all accomplished using the Docker Engine which provides a platform for images and containers to do their thing. There are two key concepts or building blocks to every docker service instance : **images** and **containers** which are defined as follows :  
  
> 🍎 Definition    
> **Images** : A Docker image is a read-only blueprint for a service (i.e., a server, a database, or any tool that your project needs). An image (as opposed to a container) does not itself *run*. It is simply the scaffolding that defines a service: including its configuration, where it stores its data, which port/s it exposes, which network/s it uses, what credentials its has access to, etc.  
>  
> These elements are brought together using a `Dockerfile` which builds a layer for each command it contains. You can either use a pre-built image pulled from the [Docker Hub](https://hub.docker.com/) or define your own `Dockerfile` that appends to a base Docker Hub image.  
>  
> Whether built from *scratch* or not, every image is based on some Linux distribution, usually Ubuntu or Alpine. This forms the base layer whereupon each consecutive command is layered on top. As such, an image can be thought of as an onion of information describing *how* a service will operate while keeping track of where in its build process each piece of information was obtained. This will be important later when determining how we wish to order the commands in the `Dockerfile`.  
>  
> Based on this layering, an image is easily cacheable; only rebuilding layers that have been changed since it was last built. That makes the process of building and rebuilding images quicker and more efficient but also serves as a sort of time capsule that can be reverted to an older layer with a simple command. This can be dangerous if you have hardcoded sensitive data into the either the `Dockerfile` or any of the configuration files as they will be congealed into the layers for anyone to find. [Learn more.](https://docs.docker.com/get-started/)  
  
> 🔮 Crystal Ball    
>We will explore the various `Dockerfile` commands and their nuances in-depth when the time comes to build our own images but this high-level overview should get you through until then.  
  
> 🍎 Definition    
> **Containers** : Once an image has been built successfully, the service is ready to be *containerized* or *run*. Docker uses the image’s instructions to run an instance of a service and does so in isolation until it is stopped and/or removed. Whatever service is being run, be it an Apache server or a MariaDB database, a container built to the exact specifications of an image will do only what is prescribed to do. It will not share state or data with any other container or service.  
>  
> Ideally each image and the container run from it are assigned a single task or mode. This is the convention for, not only, security purposes (minimizing the attack surface of a service) but also reducing the overall complexity of a single container.  
>  
> Once a container is stopped, the service stops. And once a container is removed any data that was being stored by the service is lost (though we can configure this not to be the case). Essentially, the container is ephemeral but unless specifically removed, the image will still exist and can still be run again without the need to rebuild it. This is convenient when you need to run multiple containers from a single image.  
>  
> One last thing, whereas an image builds up layer by layer of read-only data, the container applies a thin read-write layer over the top. This notion will come in handy when we start to talk about bind mounts and volumes which can persist data even when a container is stopped or removed. [Learn More](https://docs.docker.com/get-started/)  
  
As we delve deeper into the world of images and containers, I will continue drill these concepts into your brain because understanding their nature is half the battle.  
  
## 🐳 So Why Deal with Docker?  
  
There are many compelling reasons for Docker’s meteoric rise to nerd fandom :  
  
- The most obvious reason for Docker’s popularity is that you don’t need to install any programs (besides the Docker engine itself) on your **host computer**. You can also spin up a quick *utility container* to generate SSL certificates/keys (or run other specialized programs) then remove it when the task is done, all without taking up precious space on your hard drive.  
- Like a **virtual machine**, Docker allows you to develop and package a project within a nifty container. The idea of a container of code comes in handy when sharing your project. It is basically a portable version of your project that can be run anywhere and still achieve the same results.  
- Unlike a virtual machine, you can manage the size of the actual image granularly, often by using an [Alpine Linux](https://www.alpinelinux.org/) base image. Alpine Linux is super light weight because it’s not bloated with unnecessary installs or gizmos. You can also limit the image footprint with a carefully configured `Dockerfile`, multi-stage builds, and by keeping each image/container confined to a singular task.  
- Docker is also known to be great for all stages of a project, from development to production because of its scalability and integration with [Kubernetes](https://www.docker.com/products/kubernetes/), a mass container orchestration tool.  
- Docker can also be used to implement a high degree of security with its built-in [Docker Secrets](https://docs.docker.com/engine/swarm/secrets/) tool that can be used to encrypt and distribute credentials, SSL certificates, and other sensitive information to the specific containers that require them without hardcoding them into the containers themselves.  
  
> 🍎 Definition    
> **Host Computer** : The host computer is the one that you are working from right now with your own local filesystem, as opposed to the container and its internal filesystem.  
  
> 🍎 Definition    
> **Virtual Machine** : A virtual machine, much like a container, is another method of running another OS from your host computer. This is usually done using an application like [Oracle VM VirtualBox](https://www.virtualbox.org) or [Fusion by VMware](https://www.vmware.com/products/fusion.html).  
>  
> However, a major difference between a solution like Docker and that of a virtual machine is that the virtual machine exists directly on your hard drive, often requiring a partition. Furthermore, virtual machines can have a huge footprint even when running a light OS like Alpine. This is because they basically install an entire OS that often have unnecessary packages built-in.  
>  
> There are some applications for which a virtual host is more suitable, namely if you wish to emulate an entirely new OS on your host computer. But other than that, containers are quickly becoming the standard. Unpopular opinion: the age of virtual machines is behind us!  
  
If you are not convinced by now that Docker can add value to any development project, maybe this tutorial isn’t for you.. Ah well. For the rest of us believers, we should have at least a fuzzy idea of how images and containers work (in theory). All that’s left to do is install Docker and start containerizing!  
  
> 🔍 See Also    
> We haven’t even begun to scratch the surface with Docker yet! For full documentation, visit their [Learning Center](https://docs.docker.com/desktop/get-started/). [^2]  
  
## 🧰 Installing the Docker Desktop on Your Host Machine  
  
Docker will be the last installation you need to make on your host machine. Yay! The install process is fairly straight forward, just follow the instructions for your OS :  
  
- [Install Docker Desktop on Mac](https://docs.docker.com/desktop/install/mac-install/)  
- [Install Docker Desktop on Linux](https://docs.docker.com/desktop/install/linux-install/)  
- [Install Docker Desktop on Windows](https://docs.docker.com/desktop/install/windows-install/)  
  
## 🎁 Wrapping Up  
  
While that’s installing, head to the [DockerHub](https://hub.docker.com) and sign up for a free account. According to Docker, authenticated users have a higher pull rate limit (Docker, 2023). While we most likely won’t reach that limit ever... sometimes it’s better to er on the side of caution. Once signed in, go ahead and enable two-factor authentication (`Account Settings > Security > Enable 2FA`).  
  
> ✏ Note:    
> Docker will automatically log you out of Docker Desktop after 90 days, 30 days without use.[^1] It’s good practice to change you password at this time and keep them rotating.  
  
> 🌎 The Big Picture    
> While you may be averse to signing up for a free Docker account, there is another reason to do it. Later in this tutorial, we will be pushing our image to DockerHub for preservation. DockerHub allows your to host unlimited public and one private image in their cloud-based repository.  
>  
> This is useful when sharing your image with team members or, eventually, when we decide to deploy our project for production. For this you may wish to generate your first personal access token; found in the same menu as two factor authentication. Store this token and recovery code somewhere safe, outside any project directories. The token can be used in place of a password too!  
  
Once your Docker engine is revving, you will want to open Obsidian (or your note-taking app of choice) and VSCode (or your IDE of choice). We are ready to code!  
  
> 🔦 Flashback    
> If you haven’t yet installed [Obsidian](./0.1-Note-Taking-with-Obsidian.md#installing-obsidian) for taking notes like a champ or [VSCode](./0.2-Development-with-VSCode.md#installing-vs-code) for reaching IDE stardom, now would be a good time. If you have opted for your own applications for these purposes, that is fine. You will most likely still require access to a terminal with auto-completion and command history capabilities. Otherwise, you will want to die in the next Section. We will be building, running, starting, stopping, and removing, repeat until our containers run as intended.  
  
---  
  
## 📖 References  
  
[^1]: Scwarzmüller, M. (2023). Docker & Kubernetes: The Practical Guide [MOOC]. Section 5: Building Multi-Container Applications with Docker. Udemy. <https://www.udemy.com/share/103Ia03@NIeuWLCBpXz9Asqz04rkP-_CNSQIKXErlgRJiOPMlNKtC0p-elHO_4KXZAlK3wo=/>  
  
[^2]: Docker. (2023, July 8). *Learning Center and sign in*. Docker Documentation. <https://docs.docker.com/desktop/get-started/>  
